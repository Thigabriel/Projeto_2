import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.spatial import distance
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import os
import pickle

# ==============================================================================
#  CLASSE: ALMMo-1 (First Order - Regressão Linear Local)
# ==============================================================================
class ALMMo1:
    def __init__(self, raio_influencia=0.3):
        self.r = raio_influencia
        self.centers = []       
        self.counts = []        
        self.thetas = []        # Pesos da Reta (Bias, w1, w2, w3)
        self.cov_matrices = []  # Matrizes RLS
        self.n_rules = 0
        self.min_val = None
        self.max_val = None

    def _add_rule(self, x, y):
        self.centers.append(x)
        self.counts.append(1)
        
        n_features = len(x)
        theta = np.zeros(n_features + 1) 
        # Matriz de Covariância inicial (P)
        P = np.eye(n_features + 1) * 1000.0
        
        self.thetas.append(theta)
        self.cov_matrices.append(P)
        self.n_rules += 1
        
        self._update_rls(self.n_rules - 1, x, y)

    def _update_rule(self, idx, x, y):
        k = self.counts[idx]
        self.centers[idx] = ((self.centers[idx] * k) + x) / (k + 1)
        self.counts[idx] += 1
        self._update_rls(idx, x, y)

    def _update_rls(self, idx, x, y):
        x_ext = np.insert(x, 0, 1.0) # Bias
        P = self.cov_matrices[idx]
        theta = self.thetas[idx]
        
        Px = np.dot(P, x_ext)
        gain_factor = 1.0 + np.dot(x_ext.T, Px)
        K = Px / gain_factor
        
        prediction = np.dot(theta, x_ext)
        error = y - prediction
        
        self.thetas[idx] = theta + K * error
        self.cov_matrices[idx] = P - np.outer(K, x_ext.T).dot(P)

    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)
        
        # Normalização Global
        self.min_val = X.min(axis=0)
        self.max_val = X.max(axis=0)
        denom = self.max_val - self.min_val
        denom[denom == 0] = 1.0
        X_norm = (X - self.min_val) / denom

        print(f"--- Iniciando Treinamento ALMMo-1 ({len(X)} amostras) ---")
        
        for i, sample in enumerate(X_norm):
            target = y[i]

            if self.n_rules == 0:
                self._add_rule(sample, target)
                continue

            dists = distance.cdist([sample], self.centers, metric='euclidean')[0]
            nearest_idx = np.argmin(dists)
            min_dist = dists[nearest_idx]

            if min_dist > self.r:
                self._add_rule(sample, target)
            else:
                self._update_rule(nearest_idx, sample, target)
        
        print(f"Treinamento Concluído! Regras Criadas: {self.n_rules}")

    def predict(self, X):
        X = np.array(X)
        denom = self.max_val - self.min_val
        denom[denom == 0] = 1.0
        X_norm = (X - self.min_val) / denom
        preds = []

        for sample in X_norm:
            dists = distance.cdist([sample], self.centers, metric='euclidean')[0]
            similarity = 1 / (dists**2 + 1e-6)
            weights = similarity / np.sum(similarity)
            
            local_preds = []
            x_ext = np.insert(sample, 0, 1.0)
            
            for theta in self.thetas:
                val = np.dot(theta, x_ext)
                local_preds.append(val)
            
            y_pred = np.sum(weights * np.array(local_preds))
            y_pred = max(0.0, y_pred) # Proteção contra negativos
            preds.append(y_pred)
            
        return np.array(preds)

    def print_rules(self, feature_names):
        print("\n" + "="*60)
        print(" BASE DE CONHECIMENTO (ALMMo-1)")
        print("="*60)
        indices = np.argsort(self.counts)[::-1]
        for i in indices:
            if self.counts[i] < 5: continue
            center_real = self.centers[i] * (self.max_val - self.min_val) + self.min_val
            theta = self.thetas[i]
            
            print(f"REGRA #{i+1} (Peso: {self.counts[i]} dias):")
            conds = [f"{name} ≈ {val:.1f}" for name, val in zip(feature_names, center_real)]
            print(f"  SE   ({ ' E '.join(conds) })")
            
            equation = f"{theta[0]:.2f}"
            for j, coef in enumerate(theta[1:]):
                equation += f" + ({coef:.2f} * {feature_names[j]}_norm)"
            print(f"  ENTÃO Irrigação = {equation}")
            print("-" * 60)

# ==============================================================================
#  EXECUÇÃO
# ==============================================================================
arquivo_dataset = 'dataset_imperatriz_ajustado.csv'

if not os.path.exists(arquivo_dataset):
    print("Dataset não encontrado.")
    exit()

df = pd.read_csv(arquivo_dataset)

features = ['Tensao_Media_Atual_kPa', 'Chuva_Acumulada_3d_mm', 'Tmax_Futura_3d_C']
X = df[features].values
y = df['Irrigacao_mm'].values

# --- SHUFFLE LIGADO (Fundamental!) ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)

# TREINO ALMMo-1
# Teste com raio um pouco maior (0.3) para permitir que a reta se ajuste dentro da nuvem
modelo = ALMMo1(raio_influencia=0.3) 
modelo.fit(X_train, y_train)

# TESTE
y_pred = modelo.predict(X_test)

# MÉTRICAS
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"\nRESULTADOS FINAIS (ALMMo-1 com Shuffle):")
print(f"R²: {r2:.4f}")
print(f"RMSE: {rmse:.2f} mm")

modelo.print_rules(features)

# GRÁFICO
plt.figure(figsize=(15, 6))
limit = 100
plt.plot(y_test[:limit], label='Real (Net Irrigation)', color='blue', alpha=0.6, linewidth=2)
plt.plot(y_pred[:limit], label='Previsão ALMMo-1', color='red', linestyle='--', linewidth=2)
plt.title(f'Validação Final: ALMMo-1 Shuffle (R²={r2:.2f})')
plt.ylabel('Irrigação (mm)')
plt.xlabel('Amostras de Teste')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()