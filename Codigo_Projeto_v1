import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.spatial import distance
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import os

# ==============================================================================
#  CLASSE ALMMo-1 (Mantida idêntica, o segredo está nos dados agora)
# ==============================================================================
class ALMMo1:
    def __init__(self, raio_influencia=0.3):
        self.r = raio_influencia
        self.centers = []
        self.counts = []
        self.thetas = []
        self.cov_matrices = []
        self.n_rules = 0

    def _add_rule(self, x, y):
        self.centers.append(x)
        self.counts.append(1)
        n_features = len(x)
        theta = np.zeros(n_features + 1) 
        P = np.eye(n_features + 1) * 1000.0
        self.thetas.append(theta)
        self.cov_matrices.append(P)
        self.n_rules += 1
        self._update_rls(self.n_rules - 1, x, y)

    def _update_rule(self, idx, x, y):
        k = self.counts[idx]
        self.centers[idx] = ((self.centers[idx] * k) + x) / (k + 1)
        self.counts[idx] += 1
        self._update_rls(idx, x, y)

    def _update_rls(self, idx, x, y):
        x_ext = np.insert(x, 0, 1.0) 
        P = self.cov_matrices[idx]
        theta = self.thetas[idx]
        Px = np.dot(P, x_ext)
        gain_factor = 1.0 + np.dot(x_ext.T, Px)
        K = Px / gain_factor
        prediction = np.dot(theta, x_ext)
        error = y - prediction
        self.thetas[idx] = theta + K * error
        self.cov_matrices[idx] = P - np.outer(K, x_ext.T).dot(P)

    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)
        self.min_val = X.min(axis=0)
        self.max_val = X.max(axis=0)
        X_norm = (X - self.min_val) / (self.max_val - self.min_val + 1e-6)

        print(f"--- Treinando com Dataset Balanceado ({len(X)} amostras) ---")
        
        for i, sample in enumerate(X_norm):
            target = y[i]
            if self.n_rules == 0:
                self._add_rule(sample, target)
                continue
            
            dists = distance.cdist([sample], self.centers, metric='euclidean')[0]
            nearest_idx = np.argmin(dists)
            min_dist = dists[nearest_idx]

            if min_dist > self.r:
                self._add_rule(sample, target)
            else:
                self._update_rule(nearest_idx, sample, target)
        
        print(f"Treinamento Concluído! Regras Criadas: {self.n_rules}")

    def predict(self, X):
        X = np.array(X)
        X_norm = (X - self.min_val) / (self.max_val - self.min_val + 1e-6)
        preds = []
        for sample in X_norm:
            dists = distance.cdist([sample], self.centers, metric='euclidean')[0]
            similarity = 1 / (dists**2 + 1e-6)
            weights = similarity / np.sum(similarity)
            local_preds = []
            x_ext = np.insert(sample, 0, 1.0)
            for theta in self.thetas:
                local_preds.append(np.dot(theta, x_ext))
            y_pred = np.sum(weights * np.array(local_preds))
            y_pred = max(0.0, y_pred) # Irrigação não pode ser negativa
            preds.append(y_pred)
        return np.array(preds)

    def print_rules(self, feature_names):
        print("\n" + "="*60)
        print(" BASE DE CONHECIMENTO (ALMMo-1 - REGRAS DINÂMICAS)")
        print("="*60)
        indices = np.argsort(self.counts)[::-1]
        for i in indices:
            if self.counts[i] < 3: continue # Mostra regras com pelo menos 3 exemplos
            center_real = self.centers[i] * (self.max_val - self.min_val + 1e-6) + self.min_val
            theta = self.thetas[i]
            print(f"REGRA #{i+1} (Peso: {self.counts[i]}):")
            conds = [f"{name} ≈ {val:.1f}" for name, val in zip(feature_names, center_real)]
            print(f"  SE   ({ ' E '.join(conds) })")
            equation = f"{theta[0]:.2f}" 
            for j, coef in enumerate(theta[1:]):
                equation += f" + ({coef:.2f} * {feature_names[j]}_norm)"
            print(f"  ENTÃO Irrigação = {equation}")
            print("-" * 60)

# ==============================================================================
#  EXECUÇÃO COM BALANCEAMENTO
# ==============================================================================
arquivo_dataset = 'dataset_imperatriz_ajustado.csv'
if not os.path.exists(arquivo_dataset):
    print("Dataset não encontrado.")
    exit()

df = pd.read_csv(arquivo_dataset)
features = ['Tensao_Media_Atual_mbar', 'Chuva_Acumulada_3d_mm', 'Tmax_Futura_3d_C']
X = df[features].values
y = df['Irrigacao_mm'].values

# 1. Separar Treino (Cronológico) e Teste
X_train_raw, X_test, y_train_raw, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)

# 2. BALANCEAMENTO (O "Pulo do Gato")
# Vamos criar um Treino Artificial onde metade dos dias tem irrigação e metade não.
print(f"Dados Originais de Treino: {len(y_train_raw)} dias.")

# Identificar índices
indices_irrigacao = np.where(y_train_raw > 0.1)[0] # Dias que tem água
indices_seca = np.where(y_train_raw <= 0.1)[0]     # Dias de zero

print(f" - Dias COM Irrigação: {len(indices_irrigacao)}")
print(f" - Dias SEM Irrigação: {len(indices_seca)}")

# Se tivermos poucos dados de irrigação, duplicamos eles (Oversampling)
# Se tivermos muitos dados de seca, pegamos só uma parte (Undersampling)
n_samples = len(indices_irrigacao) * 2 # Vamos pegar o dobro de dias de irrigação para ter volume

# Sorteamos dias secos aleatórios para compor o resto
indices_seca_selecionados = np.random.choice(indices_seca, size=n_samples, replace=True)
indices_irrigacao_repetidos = np.random.choice(indices_irrigacao, size=n_samples, replace=True)

# Juntar tudo
indices_finais = np.concatenate([indices_irrigacao_repetidos, indices_seca_selecionados])
np.random.shuffle(indices_finais) # Embaralhar para o ALMMo não viciar

X_train_balanced = X_train_raw[indices_finais]
y_train_balanced = y_train_raw[indices_finais]

print(f"Dados Balanceados para Treino: {len(y_train_balanced)} dias (50% Com Água / 50% Sem Água).")

# 3. Treinar ALMMo-1 com dados balanceados
# Aumentei um pouco o raio para 0.35, pois agora os dados de irrigação estão densos
modelo = ALMMo1(raio_influencia=0.35) 
modelo.fit(X_train_balanced, y_train_balanced)

# 4. Testar no MUNDO REAL (Dados de Teste originais não foram mexidos)
y_pred = modelo.predict(X_test)
r2 = r2_score(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"\nRESULTADOS (Treino Balanceado -> Teste Real):")
print(f"R²: {r2:.4f}")
print(f"RMSE: {rmse:.2f} mm")

modelo.print_rules(features)

plt.figure(figsize=(15, 6))
plt.plot(y_test[:200], label='Real (Mundo Real)', color='blue', alpha=0.5)
plt.plot(y_pred[:200], label='ALMMo-1 Predição', color='red', linestyle='--', alpha=0.8)
plt.title(f'ALMMo-1 Balanceado (R²={r2:.2f})')
plt.legend()
plt.show()